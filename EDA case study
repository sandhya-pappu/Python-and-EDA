# Importing all the necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Reading dataset
df=pd.read_csv("application_data.csv")

# shape of the datset
df.shape

# Cleaning up the missing data

# listing the columns which are having more than 30% null values

emptycol=df.isnull().sum()
emptycol=emptycol[emptycol.values>(0.3*len(emptycol))]
len(emptycol)

##Analysis -1 : Apparently, there are 64 columns which are having more than 30% null values in the dataset

# Removing those 64 columns
emptycol = list(emptycol[emptycol.values>=0.3].index)
df.drop(labels=emptycol,axis=1,inplace=True)
print(len(emptycol))

# Checking the columns having lesser null percentage
df.isnull().sum()/len(df)*100

## Analysis : 2 'AMT_ANNUITY' columns is having very few null values rows. Hence let's try to impute the missing values
##Because this column is having an outlier which is very large, it will be inappropriate to fill those missing values with mean, Hence we will use Median for this and we will fill those missing banks with median value

# Filling missing values with median

values=df['AMT_ANNUITY'].median()
df.loc[df['AMT_ANNUITY'].isnull(),'AMT_ANNUITY']=values

# Removing rows having null values greater than or equal to 30%
emptyrow=df.isnull().sum(axis=1)
emptyrow=list(emptyrow[emptyrow.values>=0.3*len(df)].index)
df.drop(labels=emptyrow,axis=0,inplace=True)
print(len(emptyrow))

# Removing unwanted columns from this dataset
unwanted=['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',
       'FLAG_PHONE', 'FLAG_EMAIL','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','FLAG_EMAIL','CNT_FAM_MEMBERS', 'REGION_RATING_CLIENT',
       'REGION_RATING_CLIENT_W_CITY','DAYS_LAST_PHONE_CHANGE', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_3','FLAG_DOCUMENT_4', 'FLAG_DOCUMENT_5', 'FLAG_DOCUMENT_6',
       'FLAG_DOCUMENT_7', 'FLAG_DOCUMENT_8', 'FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_11', 'FLAG_DOCUMENT_12',
       'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', 'FLAG_DOCUMENT_15','FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_18',
       'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', 'FLAG_DOCUMENT_21']

df.drop(labels=unwanted,axis=1,inplace=True)

# Finding the categorical columns having these 'XNA' values
    
       # For Gender column
       df[df['CODE_GENDER']=='XNA'].shape

       # For Organization column
       df[df['ORGANIZATION_TYPE']=='XNA'].shape
       
       # Describing the Gender column to check the number of females and males
       df['CODE_GENDER'].value_counts()

##Observations - Since, Female is having the majority and only 4 rows are having XNA values, we can update those columns with Gender 'F' as there should not be any impact on the dataset.

# Updating the column 'CODE_GENDER' with "F" for the dataset
df.loc[df['CODE_GENDER']=='XNA','CODE_GENDER']='F'
df['CODE_GENDER'].value_counts()

# Describing the organization type column
df['ORGANIZATION_TYPE'].describe()

##Observations - So, for column 'ORGANIZATION_TYPE', we have total count of 307511 rows of which 55374 rows are having 'XNA' values. Which means 18% of the column is having this values. 
       ##Hence if we drop the rows of total 55374, should not have any major impact on our dataset.
# Hence, dropping the rows of total 55374 have 'XNA' values in the organization type column
df=df.drop(df.loc[df['ORGANIZATION_TYPE']=='XNA'].index)
df[df['ORGANIZATION_TYPE']=='XNA'].shape

# Converting all variables into numeric in the dataset
numeric_columns=['TARGET','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','REGION_POPULATION_RELATIVE','DAYS_BIRTH',
                'DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH','HOUR_APPR_PROCESS_START','LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY',
       'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY']

df[numeric_columns]=df[numeric_columns].apply(pd.to_numeric)
df.head(5)

# Creating bins for income amount
       bins = [0,25000,50000,75000,100000,125000,150000,175000,200000,225000,250000,275000,300000,325000,350000,375000,400000,425000,450000,475000,500000,10000000000]
       slot = ['0-25000', '25000-50000','50000-75000','75000,100000','100000-125000', '125000-150000', '150000-175000','175000-200000',
              '200000-225000','225000-250000','250000-275000','275000-300000','300000-325000','325000-350000','350000-375000',
              '375000-400000','400000-425000','425000-450000','450000-475000','475000-500000','500000 and above']

       df['AMT_INCOME_RANGE']=pd.cut(df['AMT_INCOME_TOTAL'],bins,labels=slot)

# Creating bins for Credit amount
       bins = [0,150000,200000,250000,300000,350000,400000,450000,500000,550000,600000,650000,700000,750000,800000,850000,900000,1000000000]
       slots = ['0-150000', '150000-200000','200000-250000', '250000-300000', '300000-350000', '350000-400000','400000-450000',
               '450000-500000','500000-550000','550000-600000','600000-650000','650000-700000','700000-750000','750000-800000',
               '800000-850000','850000-900000','900000 and above']

       df['AMT_CREDIT_RANGE']=pd.cut(df['AMT_CREDIT'],bins=bins,labels=slots)

# Dividing the dataset into two dataset of  target=1(client with payment difficulties) and target=0(client without payment difficulties)
       target0_df=df.loc[df["TARGET"]==0]
       target1_df=df.loc[df["TARGET"]==1]

# Calculating Imbalance percentage
   round(len(target0_df)/len(target1_df),2)

       ##Observation: The Imbalance ratio is 10.55
